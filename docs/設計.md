# 設計メモ

## データファイルの設計

各データタイプごとに、データファイルへのパスを書いた pathlist ファイルを使う。
データファイルへのパスはルートディレクトリからの相対パス。

同じサンプルのデータファイルは、拡張子を除いて同じ名前を付ける必要がある。

### 話者マッピングの設計

話者マッピングは `{"話者ID": ["stem1", "stem2", ...]}` の json ファイルで管理する。

## データ処理の設計

おかしなデータが来たときは帳尻合わせや正規化をするのではなく、エラーを出す。
どうしても帳尻合わせが必要な場合は、極小な誤差のみ許容して NOTE コメントとして残す。

`LazyInputData`はデータファイルのパスを保持できる。`LazyInputData.generate`ではファイルの読み込みや最低限の型変換のみを行う。
`InputData`は`preprocess`のテストがしやすいよう、実際のデータをのみを扱う。
`OutputData`はモデルやネットワークの入力できるよう、Tensor のみを扱う。

可視化や生成は学習時のデータ処理となるべく近づけるため、`Dataset`から得られる`OutputData`を使う。
`OutputData`には学習に不要な情報がないため、可視化に必要であれば`LazyInputData`からデータを取得する。

## 推論の設計

損失を計算するための経路と、生成するための経路を用意する。
損失を計算するためのネットワークが`Model`で、生成するためのネットワークが`Generator`。

## データセットの設計

データセットは 4 種類(train,test,eval,valid)ある。

train データセットは学習用で、実際に `Predictor` の重みを更新する。

test は train と同じドメインを想定し、損失のグラフからモデルが過適合していないか確認するために用いる。
train として与えられたデータの一部を test として用い、残りを真の train として用いる。

eval は test と同じデータを評価するための処理をしたもの。
データ増強をなくしたり、データ切り出し位置を固定したりする。

valid は評価のみを行うもので、train とは違うところから与えるものを想定している。
valid を用いれば、学習データが変わっても同じデータで評価できる。

## ネットワークの設計

`Tensor` や `list[Tensor]` は型として被るので、順序のみの引数で与えるのは危ない。
なのでキーワード引数としてのみでやり取りすべき。

`Predictor`含む`network`以下にあるネットワークは、学習時・生成時から使え、かつ ONNX 化もしやすいよう、Tensor などに分解されたデータを受け取るようにする。
`BatchOutput`は`Model`や`Evaluator`の入力にのみ使う。

### テンソルの shape の表記法

テンソルの引数には、コメントとして shape を記述する。

- 基本構造: `# (B, T, ?)` 形式
- 特徴量は `?` か `1` のみを使う
- バッチサイズ: `B`
- 可変長（シーケンス長）: `L`
- 特徴量方向の可変: `?`
- リストの場合: `[(L, ?)]` のように角括弧で囲む
- 出力がtupleの場合: `# (B, ?), (B,)` のように各要素をカンマで区切る
- 長さに接頭辞を付ける場合: `fL` (frame length)、`mL` (mora length)
- エンコーダ・デコーダ: `Te` (encoder)、`Td` (decoder/target)
- 複数のエンコーダ: `Ta`、`Tb` のように接頭辞を付ける
- 合計長: `sum(fL)` のように表現
- スカラー値の場合: `# ()` を書かない（コメントなし）
- 複数のテンソル引数: スカラー値以外の場合は各テンソルにコメントを書くため改行する

例：

```python
def forward(
    self,
    feature_vector: Tensor,     # (B, ?)
    feature_variable_list: list[Tensor],  # [(vL, ?)]
    speaker_id: Tensor | None,  # (B,)
):
```
